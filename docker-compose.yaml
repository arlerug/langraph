version: "3.9"

services:
  # --- LLM runtime (Ollama) opzionale se usi modelli locali ---
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    gpus: all

  # --- Vector DB ---
  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant:/qdrant/storage

  # --- HUB / router centrale ---
  central:
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn --app-dir /app central_agent:app --host 0.0.0.0 --port 8000
    environment:
      # URL interni alla network docker
      TRIAGE_URL: http://triage:8000
      BASE_AGENT_URL: http://agent_base:8000
      SPECIAL_AGENT_URL: ""   # popola quando avrai lo special
      MAX_HOPS: "5"
    depends_on:
      - triage
      - agent_base
    ports:
      - "8000:8000"  # esponi l'HUB all'host
    restart: unless-stopped

  # --- Triage (classifier snellito) ---
  triage:
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn --app-dir /app triage_agent:app --host 0.0.0.0 --port 8000
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: llama3.2:3b
    depends_on:
      - ollama
    restart: unless-stopped
    # opzionale per debug:
    ports:
      - "8010:8000"

  # --- Agente base (RAG) ---
  agent_base:
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn --app-dir /app agent:app --host 0.0.0.0 --port 8000
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: llama3.2:3b
      QDRANT_URL: http://qdrant:6333
      QDRANT_COLLECTION: kb_legale_it
      EMBED_MODEL: mxbai-embed-large
      LLM_TEMPERATURE: "0.2"
      LLM_NUM_CTX: "8192"
    depends_on:
      - ollama
      - qdrant
    restart: unless-stopped
    # opzionale per debug:
    ports:
      - "8020:8000"

  # --- UI Streamlit ---
  app:
    build:
      context: .
      dockerfile: Dockerfile
    command: streamlit run app.py --server.address=0.0.0.0 --server.port=8501
    environment:
      WESAFE_API_BASE: http://central:8000   # la UI parla col router
    depends_on:
      - central
    ports:
      - "8501:8501"
    restart: unless-stopped

volumes:
  qdrant:
  ollama:
